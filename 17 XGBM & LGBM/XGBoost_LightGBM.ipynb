{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 17: XGBoost and LightGBM\n",
    "\n",
    "## Dataset: Titanic Survival\n",
    "\n",
    "**Topics Covered:**\n",
    "- XGBoost (Extreme Gradient Boosting)\n",
    "- LightGBM (Light Gradient Boosting Machine)\n",
    "- Ensemble Methods Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Titanic_train.csv')\n",
    "print(\"Dataset loaded! Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "# Fill missing values\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "\n",
    "# Encode categorical\n",
    "df['Sex_encoded'] = LabelEncoder().fit_transform(df['Sex'])\n",
    "df['Embarked_encoded'] = LabelEncoder().fit_transform(df['Embarked'].astype(str))\n",
    "\n",
    "# Select features\n",
    "features = ['Pclass', 'Sex_encoded', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_encoded']\n",
    "X = df[features]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training:\", len(X_train), \"Testing:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "print(\"=== XGBoost ===\")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "print(\"XGBoost Accuracy:\", round(xgb_acc, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "print(\"=== LightGBM ===\")\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "lgb_pred = lgb_model.predict(X_test)\n",
    "lgb_acc = accuracy_score(y_test, lgb_pred)\n",
    "print(\"LightGBM Accuracy:\", round(lgb_acc, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "print(\"=== Model Comparison ===\")\n",
    "print(\"XGBoost:\", round(xgb_acc, 4))\n",
    "print(\"LightGBM:\", round(lgb_acc, 4))\n",
    "\n",
    "# Bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(['XGBoost', 'LightGBM'], [xgb_acc, lgb_acc], color=['blue', 'green'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('XGBoost vs LightGBM')\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# XGBoost importance\n",
    "xgb_imp = pd.DataFrame({'Feature': features, 'Importance': xgb_model.feature_importances_})\n",
    "xgb_imp = xgb_imp.sort_values('Importance')\n",
    "axes[0].barh(xgb_imp['Feature'], xgb_imp['Importance'], color='blue')\n",
    "axes[0].set_title('XGBoost Feature Importance')\n",
    "\n",
    "# LightGBM importance\n",
    "lgb_imp = pd.DataFrame({'Feature': features, 'Importance': lgb_model.feature_importances_})\n",
    "lgb_imp = lgb_imp.sort_values('Importance')\n",
    "axes[1].barh(lgb_imp['Feature'], lgb_imp['Importance'], color='green')\n",
    "axes[1].set_title('LightGBM Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Both XGBoost and LightGBM are powerful boosting algorithms\n",
    "- LightGBM is generally faster\n",
    "- Both provide feature importance rankings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
