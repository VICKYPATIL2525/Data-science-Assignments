{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Data Preprocessing and Feature Engineering\n",
    "\n",
    "## Dataset: Adult Census Income\n",
    "Predicts whether income exceeds $50K/yr based on census data.\n",
    "\n",
    "**Topics Covered:**\n",
    "- Data Exploration and Preprocessing\n",
    "- Scaling (Standard & Min-Max)\n",
    "- Encoding (One-Hot & Label)\n",
    "- Feature Engineering\n",
    "- Feature Selection (Isolation Forest, PPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('adult_with_headers.csv')\n",
    "\n",
    "# Display basic info\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "print(\"=== Data Types ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=== Missing Values ===\")\n",
    "\n",
    "# In this dataset, missing values are represented as ' ?' (with space)\n",
    "# Let's check for this pattern\n",
    "for column in df.columns:\n",
    "    # Check for ' ?' pattern\n",
    "    if df[column].dtype == 'object':\n",
    "        missing_count = (df[column].str.strip() == '?').sum()\n",
    "        if missing_count > 0:\n",
    "            print(column + \":\", missing_count, \"missing values\")\n",
    "\n",
    "# Also check for regular null values\n",
    "print(\"\\nNull values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (replace '?' with NaN then fill with mode)\n",
    "print(\"=== Handling Missing Values ===\")\n",
    "\n",
    "# Clean string columns by stripping whitespace\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        df[column] = df[column].str.strip()\n",
    "        # Replace '?' with NaN\n",
    "        df[column] = df[column].replace('?', np.nan)\n",
    "\n",
    "# Fill missing values with mode (most frequent value)\n",
    "for column in df.columns:\n",
    "    missing_count = df[column].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        mode_value = df[column].mode()[0]\n",
    "        df[column] = df[column].fillna(mode_value)\n",
    "        print(\"Filled\", missing_count, \"missing values in\", column, \"with mode:\", mode_value)\n",
    "\n",
    "print(\"\\nRemaining missing values:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Scaling Techniques\n",
    "\n",
    "**Standard Scaling:** Transforms data to have mean=0 and std=1\n",
    "- Best when: Data is normally distributed, algorithm uses distance measures\n",
    "\n",
    "**Min-Max Scaling:** Transforms data to range [0, 1]\n",
    "- Best when: You need bounded values, neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "numerical_columns = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "\n",
    "print(\"Original Data (first 5 rows):\")\n",
    "print(df[numerical_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaling\n",
    "print(\"=== Standard Scaling ===\")\n",
    "\n",
    "# Create a copy for standard scaling\n",
    "df_standard = df.copy()\n",
    "\n",
    "# Apply standard scaling manually (to show the process)\n",
    "for column in numerical_columns:\n",
    "    mean_val = df[column].mean()\n",
    "    std_val = df[column].std()\n",
    "    df_standard[column + '_std'] = (df[column] - mean_val) / std_val\n",
    "\n",
    "print(\"After Standard Scaling:\")\n",
    "std_cols = [col + '_std' for col in numerical_columns]\n",
    "print(df_standard[std_cols].head())\n",
    "\n",
    "print(\"\\nVerification (mean should be ~0, std should be ~1):\")\n",
    "for col in std_cols:\n",
    "    print(col, \"- Mean:\", round(df_standard[col].mean(), 4), \"Std:\", round(df_standard[col].std(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling\n",
    "print(\"=== Min-Max Scaling ===\")\n",
    "\n",
    "# Create a copy for min-max scaling\n",
    "df_minmax = df.copy()\n",
    "\n",
    "# Apply min-max scaling manually\n",
    "for column in numerical_columns:\n",
    "    min_val = df[column].min()\n",
    "    max_val = df[column].max()\n",
    "    df_minmax[column + '_mm'] = (df[column] - min_val) / (max_val - min_val)\n",
    "\n",
    "print(\"After Min-Max Scaling:\")\n",
    "mm_cols = [col + '_mm' for col in numerical_columns]\n",
    "print(df_minmax[mm_cols].head())\n",
    "\n",
    "print(\"\\nVerification (min should be 0, max should be 1):\")\n",
    "for col in mm_cols:\n",
    "    print(col, \"- Min:\", round(df_minmax[col].min(), 4), \"Max:\", round(df_minmax[col].max(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use which scaling:**\n",
    "\n",
    "| Standard Scaling | Min-Max Scaling |\n",
    "|-----------------|----------------|\n",
    "| Data is normally distributed | Data needs to be in [0,1] range |\n",
    "| Algorithms: SVM, Logistic Regression | Algorithms: Neural Networks, KNN |\n",
    "| Not affected by outliers much | Sensitive to outliers |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Encoding Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns = ['workclass', 'education', 'marital_status', 'occupation', \n",
    "                       'relationship', 'race', 'sex', 'native_country', 'income']\n",
    "\n",
    "# Count unique values in each categorical column\n",
    "print(\"=== Unique Values in Categorical Columns ===\")\n",
    "for column in categorical_columns:\n",
    "    unique_count = df[column].nunique()\n",
    "    print(column + \":\", unique_count, \"unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate columns based on unique values\n",
    "# Less than 5 categories -> One-Hot Encoding\n",
    "# 5 or more categories -> Label Encoding\n",
    "\n",
    "one_hot_columns = []  # < 5 categories\n",
    "label_encode_columns = []  # >= 5 categories\n",
    "\n",
    "for column in categorical_columns:\n",
    "    unique_count = df[column].nunique()\n",
    "    if unique_count < 5:\n",
    "        one_hot_columns.append(column)\n",
    "    else:\n",
    "        label_encode_columns.append(column)\n",
    "\n",
    "print(\"One-Hot Encoding columns (< 5 categories):\")\n",
    "print(one_hot_columns)\n",
    "\n",
    "print(\"\\nLabel Encoding columns (>= 5 categories):\")\n",
    "print(label_encode_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "print(\"=== One-Hot Encoding ===\")\n",
    "\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Apply one-hot encoding using pandas get_dummies\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=one_hot_columns)\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "print(\"After One-Hot Encoding:\", df_encoded.shape)\n",
    "\n",
    "# Show new columns created\n",
    "new_cols = [col for col in df_encoded.columns if col not in df.columns]\n",
    "print(\"\\nNew columns created:\")\n",
    "for col in new_cols[:10]:\n",
    "    print(\" -\", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "print(\"=== Label Encoding ===\")\n",
    "\n",
    "# Apply label encoding\n",
    "for column in label_encode_columns:\n",
    "    print(\"\\nEncoding:\", column)\n",
    "    print(\"Original values:\", df_encoded[column].unique()[:5], \"...\")\n",
    "    \n",
    "    # Create mapping dictionary\n",
    "    unique_values = df_encoded[column].unique()\n",
    "    mapping = {}\n",
    "    for i in range(len(unique_values)):\n",
    "        mapping[unique_values[i]] = i\n",
    "    \n",
    "    # Apply mapping\n",
    "    df_encoded[column] = df_encoded[column].map(mapping)\n",
    "    print(\"Encoded values:\", df_encoded[column].unique()[:5], \"...\")\n",
    "\n",
    "print(\"\\nLabel Encoding complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros and Cons:**\n",
    "\n",
    "| One-Hot Encoding | Label Encoding |\n",
    "|-----------------|----------------|\n",
    "| **Pros:** No ordinal assumption | **Pros:** Simple, no extra columns |\n",
    "| **Pros:** Works with all algorithms | **Pros:** Memory efficient |\n",
    "| **Cons:** Creates many columns | **Cons:** Implies ordinal relationship |\n",
    "| **Cons:** Memory intensive | **Cons:** May mislead some algorithms |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "print(\"=== Feature Engineering ===\")\n",
    "\n",
    "# Feature 1: Total Capital (gain - loss)\n",
    "# Rationale: Net capital change is more informative than separate gain/loss\n",
    "df['total_capital'] = df['capital_gain'] - df['capital_loss']\n",
    "print(\"Feature 1: total_capital = capital_gain - capital_loss\")\n",
    "print(\"This shows the net financial impact of capital transactions.\")\n",
    "print(\"Sample values:\", df['total_capital'].head().tolist())\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 2: Age Group\n",
    "# Rationale: Income patterns often differ by age brackets\n",
    "print(\"Feature 2: age_group\")\n",
    "print(\"Rationale: Income patterns often differ by life stages.\")\n",
    "\n",
    "# Create age groups using simple if-else logic\n",
    "age_groups = []\n",
    "for age in df['age']:\n",
    "    if age < 25:\n",
    "        age_groups.append('Young')\n",
    "    elif age < 45:\n",
    "        age_groups.append('Middle')\n",
    "    elif age < 65:\n",
    "        age_groups.append('Senior')\n",
    "    else:\n",
    "        age_groups.append('Elder')\n",
    "\n",
    "df['age_group'] = age_groups\n",
    "print(\"Age groups created:\")\n",
    "print(df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transformation on skewed feature\n",
    "print(\"=== Log Transformation ===\")\n",
    "\n",
    "# Check skewness of capital_gain\n",
    "print(\"Checking skewness of capital_gain:\")\n",
    "print(\"Mean:\", df['capital_gain'].mean())\n",
    "print(\"Median:\", df['capital_gain'].median())\n",
    "print(\"Skewness:\", df['capital_gain'].skew())\n",
    "\n",
    "# The large difference between mean and median indicates right skewness\n",
    "\n",
    "# Apply log transformation (adding 1 to avoid log(0))\n",
    "df['capital_gain_log'] = np.log1p(df['capital_gain'])\n",
    "\n",
    "print(\"\\nAfter log transformation:\")\n",
    "print(\"New skewness:\", df['capital_gain_log'].skew())\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(df['capital_gain'], bins=50, color='blue', edgecolor='black')\n",
    "axes[0].set_title('Before: capital_gain')\n",
    "axes[0].set_xlabel('Value')\n",
    "\n",
    "axes[1].hist(df['capital_gain_log'], bins=50, color='green', edgecolor='black')\n",
    "axes[1].set_title('After: capital_gain_log')\n",
    "axes[1].set_xlabel('Log Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('log_transformation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Isolation Forest for Outlier Detection\n",
    "\n",
    "**Isolation Forest** identifies outliers by isolating observations.\n",
    "Outliers are easier to isolate and thus get shorter path lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest for Outlier Detection\n",
    "print(\"=== Isolation Forest ===\")\n",
    "\n",
    "# Select numerical features for outlier detection\n",
    "features_for_outliers = df[numerical_columns].copy()\n",
    "\n",
    "# Create Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit and predict\n",
    "outlier_labels = iso_forest.fit_predict(features_for_outliers)\n",
    "\n",
    "# Count outliers (labeled as -1)\n",
    "outliers_count = 0\n",
    "inliers_count = 0\n",
    "for label in outlier_labels:\n",
    "    if label == -1:\n",
    "        outliers_count = outliers_count + 1\n",
    "    else:\n",
    "        inliers_count = inliers_count + 1\n",
    "\n",
    "print(\"Total records:\", len(outlier_labels))\n",
    "print(\"Outliers detected:\", outliers_count)\n",
    "print(\"Inliers (normal):\", inliers_count)\n",
    "print(\"Outlier percentage:\", round(outliers_count/len(outlier_labels)*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe without outliers\n",
    "df['is_outlier'] = outlier_labels\n",
    "df_clean = df[df['is_outlier'] == 1].copy()\n",
    "\n",
    "print(\"Original dataset size:\", len(df))\n",
    "print(\"After removing outliers:\", len(df_clean))\n",
    "\n",
    "print(\"\\n=== How Outliers Affect Model Performance ===\")\n",
    "print(\"1. Outliers can skew model training towards extreme values\")\n",
    "print(\"2. Distance-based algorithms (KNN, SVM) are especially sensitive\")\n",
    "print(\"3. Outliers can increase model variance and reduce generalization\")\n",
    "print(\"4. They may represent data errors or rare but valid observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "print(\"=== Correlation Matrix ===\")\n",
    "\n",
    "# Select numerical columns for correlation\n",
    "numerical_data = df[numerical_columns + ['total_capital', 'capital_gain_log']]\n",
    "\n",
    "# Calculate correlation\n",
    "correlation_matrix = numerical_data.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 PPS (Predictive Power Score)\n",
    "\n",
    "**PPS** measures how well one variable can predict another.\n",
    "Unlike correlation, it:\n",
    "- Works with categorical variables\n",
    "- Captures non-linear relationships\n",
    "- Is asymmetric (A->B may differ from B->A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple PPS-like analysis using cross-tabulation\n",
    "# Note: Full PPS requires 'ppscore' library\n",
    "print(\"=== Predictive Power Analysis ===\")\n",
    "\n",
    "# Calculate predictive relationships for categorical variables\n",
    "# Using chi-square test approach\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Check relationship between education and income\n",
    "print(\"\\nEducation vs Income:\")\n",
    "contingency_table = pd.crosstab(df['education'], df['income'])\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "print(\"Chi-square statistic:\", round(chi2, 2))\n",
    "print(\"P-value:\", p_value)\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: Strong relationship exists!\")\n",
    "\n",
    "# Check relationship between occupation and income\n",
    "print(\"\\nOccupation vs Income:\")\n",
    "contingency_table = pd.crosstab(df['occupation'], df['income'])\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "print(\"Chi-square statistic:\", round(chi2, 2))\n",
    "print(\"P-value:\", p_value)\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: Strong relationship exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare findings\n",
    "print(\"=== Comparison: Correlation vs PPS ===\")\n",
    "print()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(\"- Only measures LINEAR relationships\")\n",
    "print(\"- Works only with numerical variables\")\n",
    "print(\"- Symmetric (A,B) = (B,A)\")\n",
    "print()\n",
    "print(\"PPS (Predictive Power Score):\")\n",
    "print(\"- Measures ANY relationship (linear or non-linear)\")\n",
    "print(\"- Works with categorical AND numerical variables\")\n",
    "print(\"- Asymmetric (A predicting B may differ from B predicting A)\")\n",
    "print()\n",
    "print(\"Key Finding: Variables like education and occupation\")\n",
    "print(\"show strong predictive power for income even though\")\n",
    "print(\"they can't be measured using simple correlation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this assignment, we learned:\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Handling missing values (imputation with mode)\n",
    "   - Scaling: Standard (mean=0, std=1) vs Min-Max (range 0-1)\n",
    "\n",
    "2. **Encoding:**\n",
    "   - One-Hot: For categorical with few categories\n",
    "   - Label: For categorical with many categories\n",
    "\n",
    "3. **Feature Engineering:**\n",
    "   - Created total_capital (net capital change)\n",
    "   - Created age_group (categorized ages)\n",
    "   - Applied log transformation to reduce skewness\n",
    "\n",
    "4. **Feature Selection:**\n",
    "   - Isolation Forest to detect and remove outliers\n",
    "   - Correlation matrix for linear relationships\n",
    "   - PPS for capturing all types of relationships"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
